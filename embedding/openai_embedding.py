# -*- coding: utf-8 -*-
"""OpenAI_embedding.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Gfx72rwvJMKgE5QzLJ83wNQvKX39JzLb
"""

import openai
import numpy as np
from numpy.linalg import norm

# 各文をリストとして用意
sentences = [
    "アームをコップの近くに移動させる。",
    "ハンドを閉じる。",
    "アームを上に動かし、コップを持ち上げる。",
    "コップを目的地に移動させるか、アームを下ろす。",
    "ハンドを開き、コップを置く。"
]

# 各文の埋め込みベクトルを取得して保存
embeddings = []
for sentence in sentences:
    response = openai.embeddings.create(
        model="text-embedding-ada-002",
        input=sentence
    )
    embedding_vector = response.data[0].embedding  # 修正ポイント
    embeddings.append(embedding_vector)

# 出力: 各文の埋め込みベクトルを表示
for i, embedding in enumerate(embeddings):
    print(f"Sentence {i+1}: {sentences[i]}")
    print(f"Embedding Vector: {embedding}\n")

"""# GPT-3.5用"""

# コサイン類似度を計算する関数
def cosine_similarity(vec1, vec2):
    return np.dot(vec1, vec2) / (norm(vec1) * norm(vec2))

# 比較対象の文リスト
command = [
    "アームをコップの位置に近づける",
    "ハンドを閉じる",
    "アームを持ち上げる",
    "アームを移動する",
    "ハンドを開放する"
]

# 比較対象のコマンドリスト
wrappered_commands = [
    "ハンドを閉じる",
    "ハンドを開く",
    "アームを近づける",
    "アームを遠ざける",
    "周囲を確認する",
    "色を確認する",
    "アームを上げる",
    "行動を終了する",
    "アームを下ろす",
    "ハンドを回転させる"
]

# command の各文の埋め込みベクトルを取得
command_embeddings = []
for sentence in command:
    response = openai.embeddings.create(
        model="text-embedding-ada-002",
        input=sentence
    )
    embedding_vector = response.data[0].embedding
    command_embeddings.append(embedding_vector)

# wrappered_commands の各文の埋め込みベクトルを取得
wrappered_embeddings = []
for wrappered_command in wrappered_commands:
    response = openai.embeddings.create(
        model="text-embedding-ada-002",
        input=wrappered_command
    )
    embedding_vector = response.data[0].embedding
    wrappered_embeddings.append(embedding_vector)

# 各 command 文について、wrappered_commands 内の最も類似した文を特定
for i, command_embedding in enumerate(command_embeddings):
    most_similar_sentence = None
    highest_similarity = -1  # 初期値は最小の類似度

    # wrappered_commands 内の各文との類似度を計算
    for j, wrappered_embedding in enumerate(wrappered_embeddings):
        similarity = cosine_similarity(command_embedding, wrappered_embedding)

        # 最も類似している文と類似度を更新
        if similarity > highest_similarity:
            highest_similarity = similarity
            most_similar_sentence = wrappered_commands[j]

    # 出力: 最も類似している文とその類似度
    print(f"Command Sentence: '{command[i]}'")
    print(f"Most similar in wrappered_commands: '{most_similar_sentence}' ({highest_similarity:.4f})\n")

"""# GPT-4o用"""

# コサイン類似度を計算する関数
def cosine_similarity(vec1, vec2):
    return np.dot(vec1, vec2) / (norm(vec1) * norm(vec2))

# 比較対象の文リスト
command = [
    "ハンドを開く","ハンドを閉じる","コップを掴む","アームを下に動かす","アームを下に下げる","アームを下方向に動かす","アームをさらに下方向に動かす","アームを下降させる","アームを垂直に下げる","アームをさらに下げる","アームを持ち下げる","アームを持ち上げる","アームを左に動かす","アームを右に動かす","アームを後ろに動かす","アームを手前に動かす"," アームを前に動かす","アームを奥に動かす"
]

# 比較対象のコマンドリスト
wrappered_commands = [
    "ハンドを閉じる",
    "ハンドを開く",
    "アームを左に動かす",
    "アームを右に動かす",
    "アームを前に動かす",
    "アームを後ろに動かす",
    "アームを上げる",
    "アームを下ろす",
    "周囲を確認する",
    "ハンドを回転させる",
    "行動を終了する"

]

# command の各文の埋め込みベクトルを取得
command_embeddings = []
for sentence in command:
    response = openai.embeddings.create(
        model="text-embedding-ada-002",
        input=sentence
    )
    embedding_vector = response.data[0].embedding
    command_embeddings.append(embedding_vector)

# wrappered_commands の各文の埋め込みベクトルを取得
wrappered_embeddings = []
for wrappered_command in wrappered_commands:
    response = openai.embeddings.create(
        model="text-embedding-ada-002",
        input=wrappered_command
    )
    embedding_vector = response.data[0].embedding
    wrappered_embeddings.append(embedding_vector)

# 各 command 文について、wrappered_commands 内の最も類似した文を特定
for i, command_embedding in enumerate(command_embeddings):
    most_similar_sentence = None
    highest_similarity = -1  # 初期値は最小の類似度

    # wrappered_commands 内の各文との類似度を計算
    for j, wrappered_embedding in enumerate(wrappered_embeddings):
        similarity = cosine_similarity(command_embedding, wrappered_embedding)

        # 最も類似している文と類似度を更新
        if similarity > highest_similarity:
            highest_similarity = similarity
            most_similar_sentence = wrappered_commands[j]

    # 出力: 最も類似している文とその類似度
    print(f"Command Sentence: '{command[i]}'")
    print(f"Most similar in wrappered_commands: '{most_similar_sentence}' ({highest_similarity:.4f})\n")